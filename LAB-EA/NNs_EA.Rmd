---
title: "Training Neural Networks with Evolutionary Algorithms"
author: "Benjam√≠ Parellada and Clara Rivadulla"
date: "`r format(Sys.time(), '%d %b %Y')`"
output: 
  rmarkdown::html_document:
    toc: true
    number_sections: false
---

```{r setup, include=FALSE}
library(knitr)
library(GA)
library(tictoc)
library(nnet)
library(MASS)
library(cmaesr)

seed = 42 # 1984

opts_chunk$set(fig.align = "center", 
               out.width = "80%",
               fig.width = 6, fig.height = 5,
               dev = "svg", fig.ext = "svg",
               par = TRUE, # needed for setting hook 
               collapse = TRUE, # collapse input & output code in chunks
               warning = FALSE)

knit_hooks$set(par = function(before, options, envir)
  { if(before && options$fig.show != "none") 
       par(family = "sans", mar=c(4.1,4.1,1.1,1.1), mgp=c(3,1,0), tcl=-0.5)
})
```


## Dataset

As specified in the description of the exercise, we're going to use **synthetic data**. This will allow us to determine the sample size used for training, validation and testing, the true generalization error of a model, the amount of noise and the problem hardness.

We generate 3 different dataframes: one as the `train` set, one as the `validation` set and one as the `test` set. 

```{r}
source("generate_data.R")

data <- generate_data(3000, 0.8, 0.1, 0.1, hardness = 1, noisy = 0.5)
#data <- generate_data(3000, 0.8, 0.1, 0.1, hardness = 1, noisy = 2.5)
#data <- generate_data(3000, 0.8, 0.1, 0.1, hardness = 0, noisy = 0.5)
#data <- generate_data(3000, 0.8, 0.1, 0.1, hardness = 0, noisy = 2.5)

train <- data[[1]]
val <- data[[2]]
test <- data[[3]]
```

# Model

In order to define our **MLP** (*Multi Layer Perceptron*) model, we'll use the `nnet` package, which is intended for fitting feed-forward neural networks with a single hidden layer. 

## Genetic Algorithms For Finding the best architecture

```{r}
source("nnet_derivative.R")
Back <- train_nnet()
```

```{r}
back_ga <- Back[[2]]
Back <- Back[[1]]
plot(Back)
print(summary(Back))
back_ga
```

## Genetic Algorithms for training a neural network

```{r}
source("nnet_genetic.R")
GA <- train_ga_nnet()
```

```{r}
gs_gs <- GA[[2]]
GA <- GA[[1]]
plot(GA)
print(summary(GA))
gs_gs
```

## Evolution Strategies

```{r}
source("nnet_evolutionary.R")
es_es <- train_es_nnet()
```

```{r}
es_es
```


# Results

```{r}
print(rbind(back_ga, gs_gs, es_es), row.names = FALSE)
```

